{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f999cfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://5df2e831444a:4040\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1633658133253)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"./people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd87f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9564ef83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at <console>:25\n",
       "wordsDF: org.apache.spark.sql.DataFrame = [value: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordsRDD = sc.parallelize(Array(\"a\", \"b\", \"c\", \"d\", \"a\", \"a\", \"b\", \"b\", \"c\", \"d\", \"d\", \"d\", \"d\"))\n",
    "val wordsDF = wordsRDD.toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f98fcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    a|\n",
      "|    b|\n",
      "|    c|\n",
      "|    d|\n",
      "|    a|\n",
      "|    a|\n",
      "|    b|\n",
      "|    b|\n",
      "|    c|\n",
      "|    d|\n",
      "|    d|\n",
      "|    d|\n",
      "|    d|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34431e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordsDF: org.apache.spark.sql.DataFrame = [word: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordsDF = wordsRDD.toDF(\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327068c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "|   a|\n",
      "|   b|\n",
      "|   c|\n",
      "|   d|\n",
      "|   a|\n",
      "|   a|\n",
      "|   b|\n",
      "|   b|\n",
      "|   c|\n",
      "|   d|\n",
      "|   d|\n",
      "|   d|\n",
      "|   d|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc510c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleRDD: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[15] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleRDD = sc.parallelize(\n",
    "  Seq( (\"David\", 150),\n",
    "       (\"White\", 200),\n",
    "       (\"Paul\",  170) )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606d241d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleDF: org.apache.spark.sql.DataFrame = [name: string, salary: int]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDF = peopleRDD.toDF(\"name\", \"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14076371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|David|   150|\n",
      "|White|   200|\n",
      "| Paul|   170|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fbe4953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n",
       "peopleRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[19] at parallelize at <console>:29\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// RDD를 Row 로 초기화 \n",
    "val peopleRDD = sc.parallelize(\n",
    "  Seq(\n",
    "       Row(\"David\", 150),\n",
    "       Row(\"White\", 200),\n",
    "       Row(\"Paul\",  170)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6d82cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleSchema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(salary,IntegerType,true))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleSchema = new StructType().add(StructField(\"name\",   StringType, true)).add(StructField(\"salary\", IntegerType, true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fc7b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleDF: org.apache.spark.sql.DataFrame = [name: string, salary: int]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDF = spark.createDataFrame(peopleRDD, peopleSchema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74f0ef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|David|   150|\n",
      "|White|   200|\n",
      "| Paul|   170|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658ccbc",
   "metadata": {},
   "source": [
    "#  https://wikidocs.net/28556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ddec663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq: Seq[(String, Int)] = List((David,150), (White,200), (Paul,170))\n",
       "peopleDS: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seq =   Seq(\n",
    "       (\"David\", 150),\n",
    "       (\"White\", 200),\n",
    "       (\"Paul\",  170)\n",
    "  )\n",
    "val peopleDS = seq.toDS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6af9b59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|David|150|\n",
      "|White|200|\n",
      "| Paul|170|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a65cc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   _1|\n",
      "+-----+\n",
      "|David|\n",
      "|White|\n",
      "| Paul|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.select(\"_1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2646c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class People\n",
       "peopleSeq: Seq[People] = List(People(David,150), People(White,200), People(Paul,170))\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class People(name: String, salary: Int)\n",
    "val peopleSeq = Seq(\n",
    "       People(\"David\", 150),\n",
    "       People(\"White\", 200),\n",
    "       People(\"Paul\",  170)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e11b1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleDS: org.apache.spark.sql.Dataset[People] = [name: string, salary: int]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDS = peopleSeq.toDS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "609111c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|David|   150|\n",
      "|White|   200|\n",
      "| Paul|   170|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46d6ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|   150|\n",
      "|   200|\n",
      "|   170|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.select(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e29344d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d02ea801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleRDD: org.apache.spark.rdd.RDD[String] = ./people.txt MapPartitionsRDD[24] at textFile at <console>:37\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val peopleRDD = sc.textFile(\"./people.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbffbb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleSchema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true))\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleSchema = new StructType().add(StructField(\"name\",   StringType, true)).add(StructField(\"age\", IntegerType, true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad3ae368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepPeopleRdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[26] at map at <console>:38\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sepPeopleRdd = peopleRDD.map(line => line.split(\",\")).map(x => Row(x(0), x(1).trim.toInt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2332f4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleDF: org.apache.spark.sql.DataFrame = [name: string, age: int]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDF = spark.createDataFrame(sepPeopleRdd, peopleSchema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18681d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   A| 29|\n",
      "|   B| 30|\n",
      "|   C| 19|\n",
      "|   D| 15|\n",
      "|   F| 20|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ddf0b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class People\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class People(name: String, age: Long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50e94181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleDS: org.apache.spark.sql.Dataset[People] = [name: string, age: int]\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDS = peopleDF.as[People]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd4bcde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   A| 29|\n",
      "|   B| 30|\n",
      "|   C| 19|\n",
      "|   D| 15|\n",
      "|   F| 20|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2095413e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class People\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class People(name: String, age: Long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8866977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDF = spark.read.json(\"./people.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "360a63d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleDS: org.apache.spark.sql.Dataset[People] = [age: bigint, name: string]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDS = peopleDF.as[People]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9486386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f083d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class People\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class People(name: String, age: Long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adf769cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDF = spark.read.json(\"./people.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7931fba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peopleDS: org.apache.spark.sql.Dataset[People] = [age: bigint, name: string]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDS = peopleDF.as[People]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c453cef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbf02989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0b9e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7b63f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.select($\"name\", $\"age\" + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cb95c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.filter(\"age is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a7c6f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Andy| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.select($\"name\", $\"age\").filter($\"age\" > 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9046e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Andy| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDS.select($\"name\", $\"age\").filter(\"age > 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ec32621",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 32) (5df2e831444a executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @20e2b45d; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @37c27db9)",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 32) (5df2e831444a executor driver): java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @20e2b45d; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @37c27db9)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:793)",
      "  ... 44 elided",
      "Caused by: java.lang.ClassCastException: class $iw cannot be cast to class $iw ($iw is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @20e2b45d; $iw is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @37c27db9)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "peopleDS.filter(\"age is not null\").map(p=>p.name+\",\"+p.age).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7410732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79408c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59008d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad407ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9f82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b489f2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511c019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6badf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f926277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd4078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81826ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbbb711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
