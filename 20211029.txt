$ sudo apt install scala

$ nano $HOME/.bashrc
export SCALA_HOME=/usr/bin

control-O/control-X

$ source $HOME/.bashrc
$ echo $SCALA_HOME
$ scala -version

$ wget https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
$ tar xvfz  spark-3.2.0-bin-hadoop3.2.tgz
$ cd spark-3.2.0-bin-hadoop3.2

$ nano $HOME/.bashrc
export SPARK_HOME=$HOME/spark-3.2.0-bin-hadoop3.2
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

$ cd $SPARK_HOME/conf
$ cp spark-env.sh.template spark-env.sh

$ nano spark-env.sh
export SPARK_HOME=$HOME/spark-3.2.0-bin-hadoop3.2
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

$ start-dfs.sh
$ start-master.sh
$ start-workers.sh
$ jps

$ spark-shell

우분투 홈폴더(ubuntu)
$HOME -> /home/ubuntu

HDFS상에 /user/ubuntu

$ hdfs dfs -mkdir /user
$ hdfs dfs -mkdir /user/ubuntu
$ hdfs dfs -put $SPARK_HOME/README.md /user/ubuntu
$ hdfs dfs -ls /user/ubuntu

$ spark-shell
scala> val textFile = sc.textFile("hdfs://127.0.0.1:9000/user/ubuntu/README.md")
scala> textFile.count()
         
val textFile = sc.textFile("hdfs://127.0.0.1:9000/user/ubuntu/README.md")
val counts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_)
counts.saveAsTextFile("hdfs://127.0.0.1:9000/user/ubuntu/mapreduce_result_spark_shell2")

:quit

$ hdfs dfs -cat /user/ubuntu/mapreduce_result_spark_shell/part-0000?

- wiki데이터를 워드카운트 실행

$ docker start xxxxx
$ docker logs xxxxx

import org.apache.spark._
import org.apache.spark.streaming._

val ssc = new StreamingContext(sc, Seconds(5))      // 5초간격 배치 처리 
val lines = ssc.socketTextStream("127.0.0.1", 9999) // 텍스트를 입력 받을 IP, port 입력 
val words = lines.flatMap(_.split(" "))
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.print()

ssc.start()   // 데이터 받기 시작 
ssc.awaitTermination() 

창을 새로 띄우고

$ docker exec -it xxxx bash

# nc -n 123.45.67.89 -l 9999

실시간집계(인기투표) -> 네이버 실시간 검색어

1 김개똥
2 홍길동

val ssc = new StreamingContext(sc, Seconds(5))
val lines = ssc.socketTextStream("127.0.0.1", 9999)
val words = lines.flatMap(_.split(" "))
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)

val wordCountWindow = wordCounts.reduceByKeyAndWindow( {(x, y) => x + y}, {(x, y) => x - y}, Seconds(30), Seconds(10))
wordCountWindow.print()


Buffer(=Queue) vs. Cache

입출력속도의 "갭 완충"
producer >= consumer

first-in first-out => FIFO/LILO
- 손실 free ...

Cache => Locality(지역성 시간적/공간적)
  메모리 속도가 상이 => mix(빠른메모리+느리메모리)해서 빠른 메모리에 가까운 속도
  cf. 메모리 계층구조
  블랜딩(blending) -> 커피(90%) - specialty / 양주(블렌디드 양주, 글랜피딕)

  제일 빠른 메모리 -> 레지스터(64비트/10-20) - L1/L2(코어별)/L3(공유-가격차이)

  캐시 VS. 메모리 
     100번지 메모리내용을 읽어(8바이트 읽기) -> Cache(101,102,103,104 미리 예측) hit/miss 85-90%

  캐시 교체정책(Replacement Policy)
  - LRU(Least Recently Used) / LFU(Least Frequently Used)
    hit시간 / hit카운트 

  캐시 업데이트 정책

  1. 동시에 업데이트(Write Throught)
  2. 캐시만 우선 업데이트하고 메모리는 나중에 업데이트(Write-back)



 

이중화/다중화/복제(Replication)

- 읽기속도는 n배 향상
- 쓰기속도는 완만한 감소

=> 웹포털(Naver, Daum, Nate, ...)

빠른 쓰기속도가 필요한 경우 -> 파티션/샤딩


읽기/쓰기 속도는 동시에 개선
1. 샤딩(파티셔닝) 2. 파티션(샤딩)을 여러 개 복제
 













